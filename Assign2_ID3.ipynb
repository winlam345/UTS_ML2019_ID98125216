{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assign2_ID3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winlam345/UTS_ML2019_ID98125216/blob/master/Assign2_ID3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHgWxe62rrEF",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 2: Pratical Machine Learning Project\n",
        "\n",
        "## Introduction\n",
        "\n",
        "ID3 short for iterative Dichotomiser 3, is a decision tree learning algorithm created by Ross Quinlan. As a learning algorithm, ID3 uses labelled data to construct decision trees that later can be used for classification of unseen data. \n",
        "\n",
        "ID3 is unique in that it is a greedy algorithm. This means that the algorithm constructs a decision tree in a top down manner; starting from the root down to the leaf nodes where each successive addition is chosen as best option at the time. As such, the decision tree produced is unlikely to be most optimal classifer.\n",
        "\n",
        "ID3 uses information gain as a metric to determine the best condition to separate the dataset. Information gain is the reduction of overall entropy.\n",
        "\n",
        "### Input\n",
        "This implementation of the ID3 algorithm requires the user to provide the column name of the class label and a dataset. The dataset is split into a training set and test set respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7bN4IJioqmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "# userinput: change sns.load_dataset('iris') to desired dataset\n",
        "dataset = sns.load_dataset('iris')\n",
        "train_set, test_set = create_train_test(dataset, 0.8)\n",
        "\n",
        "# user input: change 'species' to dsired class label\n",
        "target_attribute = 'species'\n",
        "\n",
        "decision_tree = train_tree(train_set, target_attribute) # need to run from methodology onward first"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-bYXKJPou_n",
        "colab_type": "text"
      },
      "source": [
        "### Output\n",
        "This implemenation outputs a tree structure that can be used to find accuracy value and provide modified  appends prediction to test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W71OxoE5qeeM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c575ff89-1402-457e-b7ed-357a47d1600b"
      },
      "source": [
        "print(decision_tree)\n",
        "print(accuracy(test_set, tree, target_attribute))\n",
        "print(test_set.head())"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'petal_width <= 0.5': ['setosa', {'petal_length <= 6.4': ['versicolor', 'virginica']}]}\n",
            "0.7666666666666667\n",
            "    sepal_length  sepal_width  petal_length  ...  species prediction comparison\n",
            "9            4.9          3.1           1.5  ...   setosa     setosa       True\n",
            "14           5.8          4.0           1.2  ...   setosa     setosa       True\n",
            "19           5.1          3.8           1.5  ...   setosa     setosa       True\n",
            "21           5.1          3.7           1.5  ...   setosa     setosa       True\n",
            "25           5.0          3.0           1.6  ...   setosa     setosa       True\n",
            "\n",
            "[5 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joTMVxuIszgo",
        "colab_type": "text"
      },
      "source": [
        "## Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni_gXWuz1OD2",
        "colab_type": "text"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGi1Ql7-1Wu6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "86860a81-5ddd-4b8e-bd0c-c8db58f2640f"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            "sepal_length    150 non-null float64\n",
            "sepal_width     150 non-null float64\n",
            "petal_length    150 non-null float64\n",
            "petal_width     150 non-null float64\n",
            "species         150 non-null object\n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 5.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPrSlWYE1kpo",
        "colab_type": "text"
      },
      "source": [
        "The iris flower dataset is a well known dataset that is widely used for educational and demonstrative purposes for teaching machine learning. As such all 150 instances have no null values and the entire dataset requires little preprocessing. This is useful for focusing on a implementation of the ID3 algorithm as there is no need to remove or estimate missing values.\n",
        "\n",
        "The dataset itself contains various measurements of various iris flowers organised into four attributes (sepal_length, sepal_width, petal_width, petal_length, species) and one class label (species). The class labels are not continous values meaning a classification tree is needed rather than a regression tree. This will mean that this dataset can test make use of implemented ID3 aglorithm. But the attributes values are continous values and requires modification in the implementation to handle these cases.\n",
        "\n",
        "## Challenges\n",
        "The intial challenges encountered in the implementation of ID3 algorithm was how to represent a tree structure in python. There are many methods to represent a tree structure in python. As a novice programmer building a node class and node functionality was quite difficult to grasp. Being not able to handle further errors or bugs required me to find a simplier solution to represent trees for ID3 algorithm. A choice was to restrict the underlying tree structure of the decision tree to a binary tree. A binary tree is a type of tree structure that limits the amount of child nodes a parent node can have to two. This would also help with handling continous data attributes. \n",
        "\n",
        "### Data Structure\n",
        "\n",
        "The datastructure binary tree of implementation would constructed using dictionaries. A dictionary would respresent a sub-tree where the key would be the parent node and the value would be a two element list to represent two possible children nodes. To represent depth in a binary tree, one of the child nodes may also be a sub-tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvZkqC_W38XA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = {'key': ['element_1', 'element_2']}\n",
        "#binary_tree = {parent_node: [child_1, child_2]}\n",
        "#depth2 tree\n",
        "#binary_tree = {parent_node_1: [child_1, {parent_node_2: [child_2, child_3]} ]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Po2vmntfoI",
        "colab_type": "text"
      },
      "source": [
        "## Methdology \n",
        "\n",
        "### ID3 psuedo code provided"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeyDi5CBA70d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d8a8b8dd-a3a9-4ef9-86c5-eafa039e04a3"
      },
      "source": [
        "\"\"\"\n",
        "Pseudocode ID3 (Examples, Target_Attribute, Attributes)\n",
        "\n",
        "  Create a root node for the tree\n",
        "\n",
        "  If all examples are positive, Return the single-node tree Root, with label = +.\n",
        "  If all examples are negative, Return the single-node tree Root, with label = -.\n",
        "  If number of predicting attributes is empty, then Return the single node tree Root, with label = most common value of the target attribute in the examples.\n",
        "\n",
        "  Otherwise Begin\n",
        "\n",
        "      \n",
        "      A ← The Attribute that best classifies examples.\n",
        "      Decision Tree attribute for Root = A.\n",
        "\n",
        "      For each possible value, vi, of A,\n",
        "          Add a new tree branch below Root, corresponding to the test A = vi.\n",
        "\n",
        "          Let Examples(vi) be the subset of examples that have the value vi for A\n",
        "\n",
        "          If Examples(vi) is empty\n",
        "\n",
        "              Then below this new branch add a leaf node with label = most common target value in the examples\n",
        "\n",
        "          Else below this new branch add the subtree ID3 (Examples(vi), Target_Attribute, Attributes – {A})\n",
        "\n",
        "  End\n",
        "  Return Root\n",
        "\"\"\""
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPseudocode ID3 (Examples, Target_Attribute, Attributes)\\n\\n  Create a root node for the tree\\n\\n  If all examples are positive, Return the single-node tree Root, with label = +.\\n\\n  If all examples are negative, Return the single-node tree Root, with label = -.\\n\\n  If number of predicting attributes is empty, then Return the single node tree Root, with label = most common value of the target attribute in the examples.\\n\\n  Otherwise Begin\\n\\n      A ← The Attribute that best classifies examples.\\n      Decision Tree attribute for Root = A.\\n\\n      For each possible value, vi, of A,\\n          Add a new tree branch below Root, corresponding to the test A = vi.\\n\\n          Let Examples(vi) be the subset of examples that have the value vi for A\\n\\n          If Examples(vi) is empty\\n\\n              Then below this new branch add a leaf node with label = most common target value in the examples\\n\\n          Else below this new branch add the subtree ID3 (Examples(vi), Target_Attribute, Attributes – {A})\\n\\n  End\\n  Return Root\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y8m7ei0A5jq",
        "colab_type": "text"
      },
      "source": [
        "Pseudo Code was understood as in the following images:\n",
        "\n",
        "https://drive.google.com/file/d/1_R56zO81NUfBt83mJyKf4g_gpMnS7rIm/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIYnjhIABK7K",
        "colab_type": "text"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uB_qKowvO6g",
        "colab_type": "text"
      },
      "source": [
        "### Import Statements\n",
        "\n",
        "--- Start run here(Ctrl+F10)---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgGwQopGsRSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "dataset = sns.load_dataset('iris')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GxgQgeBvHIg",
        "colab_type": "text"
      },
      "source": [
        "### Create Train and Test Datasets\n",
        "\n",
        "To perform classification with a decision tree using ID3, the dataset must be first be split into training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgPw5097vLVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_train_test(dataset, split_ratio):\n",
        "  \"\"\"\n",
        "  Generates two subets from the main dataset according to specified a ratio\n",
        "  \"\"\"\n",
        "  dataset_copy = dataset\n",
        "  training_set = dataset_copy.sample(frac = split_ratio, random_state=0)\n",
        "  testing_set = dataset_copy.drop(training_set.index)\n",
        "    \n",
        "  return training_set, testing_set\n",
        "\n",
        "train_set, test_set = create_train_test(dataset, 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj5VBH5JxwQy",
        "colab_type": "text"
      },
      "source": [
        "## ID3 Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FpdqWEHvpH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def id3(dataset, attributes, target_attribute):\n",
        "  \"\"\"\n",
        "  There are param for id3 algorithm.\n",
        "  dataset: dataframe of current subset\n",
        "  attributes: list of attributes found in current subset\n",
        "  target_attribute: string of class label\n",
        "  \"\"\"\n",
        "  \n",
        "  # condition_1 to stop recursion\n",
        "  if check_homogenous(dataset, target_attribute):\n",
        "    class_labels = dataset[target_attribute].unique()\n",
        "    return class_labels[0]\n",
        "  \n",
        "  else:\n",
        "      \n",
        "      # identify all possible values to seperate dataset on\n",
        "      potential_splits = find_potential_splits(attributes, dataset)\n",
        "      \n",
        "      # find best attribute and attribute value out of possible splits\n",
        "      best_attribute, best_attribute_value = find_best_split(dataset, attributes, target_attribute, potential_splits)\n",
        "      \n",
        "      #seperate the data accordingly\n",
        "      less_equal_than, greater_than = split_data(dataset, best_attribute, best_attribute_value)\n",
        "        \n",
        "      # condition_2 to stop recursion\n",
        "      if check_empty(less_equal_than, greater_than):\n",
        "        leaf_node = make_leaf_node(dataset, target_attribute)\n",
        "        return leaf_node\n",
        "      \n",
        "      #construct tree \n",
        "      split_condition = \"{} <= {}\".format(best_attribute, best_attribute_value)\n",
        "      binary_tree = {split_condition: []}\n",
        "      \n",
        "      # recursion\n",
        "      satisfy_subset = id3(less_equal_than, attributes, target_attribute)\n",
        "      failed_subset = id3(greater_than, attributes, target_attribute)\n",
        "      binary_tree[split_condition].append(satisfy_subset)\n",
        "      binary_tree[split_condition].append(failed_subset)\n",
        "      \n",
        "      return binary_tree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzeUJgGgESHR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5K64f9R0Cuq",
        "colab_type": "text"
      },
      "source": [
        "### Condition_1\n",
        "\n",
        "\"if every element in the subset belongs to the same class; in which case the node is turned into a leaf node and labelled with the class of the examples\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PynHZgGSzqBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_homogenous(dataset, target_attribute):\n",
        "    \"\"\"\n",
        "    Asks if the current dataset only contains one class label value\n",
        "    \"\"\"\n",
        "    class_labels = dataset[target_attribute].unique()\n",
        "    if len(class_labels) == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWhKJeR748LC",
        "colab_type": "text"
      },
      "source": [
        "### Condition_2\n",
        "\n",
        "\"there are no more attributes to be selected, but the examples still do not belong to the same class. In this case, the node is made a leaf node and labelled with the most common class of the examples in the subset.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeDBFDgH48dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_empty(subset, subset_2):\n",
        "  \"\"\"\n",
        "  asks if there are any more attributes in the subsets to be selected\n",
        "  \"\"\"\n",
        "  if (len(subset) == 0 or len(subset_2) == 0):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXhWZGsN5viT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_leaf_node(dataset, target_attribute):\n",
        "  \"\"\"\n",
        "  returns the most common class label in the current dataset and turn into leaf node\n",
        "  \"\"\"\n",
        "  class_labels, class_labels_count = np.unique(dataset[target_attribute], return_counts=True)\n",
        "    \n",
        "  index = class_labels_count.argmax()\n",
        "  leaf_node = class_labels[index]\n",
        "    \n",
        "  return leaf_node"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME42dzHg1EAa",
        "colab_type": "text"
      },
      "source": [
        "### find_potential_splits(attribute, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8Y__lTu1V2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_potential_splits(attribute_list, dataset):\n",
        "  \"\"\"\n",
        "  returns a dictionary of all unique possible values\n",
        "  key: attribute\n",
        "  value: list of corresponding attribute values\n",
        "  \"\"\"\n",
        "  potential_splits = {}\n",
        "  \n",
        "  for i in range(len(attribute_list)):\n",
        "    potential_splits[attribute_list[i]] = []\n",
        "    unique_values = train_set[attribute_list[i]].unique()\n",
        "    potential_splits[attribute_list[i]].append(unique_values)\n",
        "        \n",
        "  return potential_splits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIhSaI7R2kNg",
        "colab_type": "text"
      },
      "source": [
        "### Split_Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCmv5Zt_2kbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(dataset, best_attribute, best_value):\n",
        "  \"\"\"\n",
        "  takes current dataset and binary split into two subset\n",
        "  \"\"\"\n",
        "  values = dataset[best_attribute]\n",
        "  less_equal_than = dataset[values <= best_value]\n",
        "  greater_than = dataset[values > best_value]\n",
        "    \n",
        "  return less_equal_than, greater_than"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl4gfiIk3QPk",
        "colab_type": "text"
      },
      "source": [
        "### Find_bestsplit\n",
        "\n",
        "Find best attribute and best attribute value using information gain as the metric to compare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LCxhySe3-Gn",
        "colab_type": "text"
      },
      "source": [
        "### Entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmR_-z-M3-Vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entropy(dataset, target_attribute):\n",
        "  class_labels = dataset[target_attribute]\n",
        "  _, counts = np.unique(class_labels, return_counts = True)\n",
        "    \n",
        "  probabilities = counts / counts.sum()\n",
        "    \n",
        "  entropy = sum(probabilities * -np.log2(probabilities))\n",
        "    \n",
        "  return(entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7Fvxwd13-tt",
        "colab_type": "text"
      },
      "source": [
        "### Information Gain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6H2ZtE23-33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def information_gain(less_equal_than, greater_than, target_attribute):\n",
        "    total_data = len(less_equal_than) + len(greater_than)\n",
        "    prob_less_equal_than = len(less_equal_than) / total_data\n",
        "    prob_greater_than = len(greater_than) / total_data\n",
        "    \n",
        "    information_gain = prob_less_equal_than * entropy(less_equal_than, target_attribute) + (prob_greater_than * entropy(greater_than, target_attribute))\n",
        "    \n",
        "    return information_gain\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu6ZDg_73Qi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_best_split(dataset, attribute_list, target_attribute, potential_splits):\n",
        "  \"\"\"\n",
        "  # find best attribute and attribute value out of possible splits\n",
        "  # best is highest information gain\n",
        "  \"\"\"\n",
        "  best_info_gain = 99999 # arbituary large number\n",
        "    \n",
        "  for attribute in potential_splits:\n",
        "      for dict_value in potential_splits[attribute]:\n",
        "          #taking array object out of dictionary value\n",
        "          attribute_values = dict_value\n",
        "          for value in attribute_values:\n",
        "              less_equal_than, greater_than = split_data(dataset, attribute, value)\n",
        "              current_gain = information_gain(less_equal_than, greater_than, target_attribute)\n",
        "                \n",
        "          if current_gain <= best_info_gain :\n",
        "              best_info_gain = current_gain\n",
        "              best_attribute = attribute\n",
        "              best_attribute_value = value\n",
        "    \n",
        "    \n",
        "  return best_attribute, best_attribute_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8zbluaG9vA6",
        "colab_type": "text"
      },
      "source": [
        "# Train a decision tree using ID3\n",
        "Main function to call to create decision tree by user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZoRg99i7-hM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_tree(train_set, target_attribute):\n",
        "  \"\"\"\n",
        "  takes user input and prepares parameters\n",
        "  \"\"\"\n",
        "  attribute = list(train_set.columns)\n",
        "  attribute.remove(target_attribute)\n",
        "  \n",
        "  decision_tree = id3(train_set, attribute, target_attribute)\n",
        "\n",
        "  return decision_tree\n",
        "\n",
        "#tree = train_tree(train_set, 'species')\n",
        "#print(tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFu4K6OS_E2E",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WgXNmdHq61L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(instance, tree):\n",
        "    \"\"\"\n",
        "    compare prediction of tree with test data\n",
        "    \"\"\"\n",
        "    split_condition = list(tree.keys())[0]\n",
        "    attribute, operand, value = split_condition.split()\n",
        "    \n",
        "    if instance[attribute] <= float(value):\n",
        "        label = tree[split_condition][0]\n",
        "    else:\n",
        "        label = tree[split_condition][1]\n",
        "    \n",
        "    if not isinstance(label, dict):\n",
        "        return label\n",
        "    else:\n",
        "        remaining_tree = label\n",
        "        return classify(instance, remaining_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5USc9b9K9Biu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(unknown_dataset, tree, target_attribute):\n",
        "    #create column of tree's prediction\n",
        "    unknown_dataset['prediction'] = unknown_dataset.apply(classify,  args=(tree,), axis=1)\n",
        "    unknown_dataset['comparison'] = unknown_dataset.prediction == unknown_dataset[target_attribute]\n",
        "                    \n",
        "    accuracy = unknown_dataset.comparison.mean()\n",
        "                    \n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC56dx55q-RC",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEyQzl7S-6dr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94ce3e33-7937-4351-b5fc-f9dd44b1448f"
      },
      "source": [
        "decision_tree = train_tree(train_set, target_attribute)\n",
        "accuracy(test_set, tree, target_attribute)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7666666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUuN268iJR5a",
        "colab_type": "text"
      },
      "source": [
        "## Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-ENdALJrQl-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8e65b6df-bd41-4cb1-8bfb-3857d2a39363"
      },
      "source": [
        "time = %time decision_tree = train_tree(train_set, target_attribute)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 583 ms, sys: 2.46 ms, total: 586 ms\n",
            "Wall time: 595 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_0Jg3koNs2t",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vunWMhC3JiSZ",
        "colab_type": "text"
      },
      "source": [
        "Overall the accuracy of my implementation of ID3 is rather low with accuracy of 77%. The run time is fast but the dataset is rather small only containg few instances. \n",
        "\n",
        "To futher improve this implementation extra features of maximum depth and could be implemented. Maximum depth would limit how deep a tree may be and be used as tuning parameter to prevent over fitting the model. \n",
        "\n",
        "Another improvement to implementation is to properly implement a tree data structure rather than just a binary tree. This would allow for possibility for many splits to occur per split condition.\n",
        "\n",
        "Other improvements would be to include a data split function that includes a cross validation dataset and fit function to optimise any future implemented parameters of this algorithm. This would allow for construction of better models on complicated data and reduce the chance of over-fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJec_hmYNzWR",
        "colab_type": "text"
      },
      "source": [
        "# Ethics\n",
        "\n",
        "Ethics is defined as the moral principles that govern a person's behaviour or the conducting of an activity. As such to look we must look at possible applications of a decision tree and the implications of their use. Decision trees can effectively model possible decisions and outcomes and such are used in decision making applications. These applications can in theory vary from very pragmatic uses such as churn analysis and marketing selection to very serious that involve patient medical data. Decision trees may be used to diagnose patient or administer medications according to symptoms. In its nature, decision trees and machine learning operate probablistically and will contain errors. In the event that an error occurs that puts a person's life at risk provided by a decision tree, is this acceptable? \n",
        "\n",
        "From a utilitarian approach, the good of the many is superior the the good of the few. In theory, a commercial decision algorithm applicationn will have rather low error rate. Although few people are put at risk, a larger population of people will benefit from use of decision trees and as such is it acceptable to use decision trees to handle life or death situations from a ultilitarian view."
      ]
    }
  ]
}